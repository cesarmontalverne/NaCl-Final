---
title: "Alex"
author: "Nathan, Alex, Caesar, Liam"
date: "2022-12-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
#install.packages("plyr")
library(plyr)
library(pROC)
library(plotly)
library(randomForest)
library(mltools)
library(rio)
library(caret)
library(ROCR)
library(tidyverse)
library(data.table)
library(rpart)
#install.packages("pscyh")
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
```


```{r}
cluster3 <- read.csv("cluster_3_data.csv")
cluster3 <- cluster3 %>% select(-c("clusterNum", "state", "county"))
```


Create test, tune and training sets 
```{r}
set.seed(1)
# 85% train 15% for tune and test, try to give more data for random forest training
part_index_1 <- createDataPartition(cluster3$obesrate,
                                           times=1,
                                           p = 0.85,
                                           groups=1,
                                           list=FALSE)
train <- cluster3[part_index_1,]
test <- cluster3[-part_index_1, ]
```

Calculate the initial mtry level 
```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
mytry_tune(cluster3) # 2.236068 -> 2
```

```{r}
set.seed(1)
cluster3_RF = randomForest(obesrate~., train, ntree = 100,
                            mtry = 2,            
                            replace = TRUE,      
                            sampsize = 100,      
                            nodesize = 5,        
                            importance = TRUE,   
                            proximity = FALSE,    
                            norm.votes = TRUE,   
                            do.trace = TRUE,     
                            keep.forest = TRUE,  
                            keep.inbag = TRUE)   

# Settled on ntrees=100 since it yielded the best MSE while keeping sample size capped at 100 as well. Didn't want to increase sample size too much even if it led to a lower MSE because I don't want the model to overfit to the data. 
```

```{r}
View(as.data.frame(cluster3_RF$importance))

# Diabetes was the most important in predicting the obesity rate which makes sense since these would appear to be highly correlated variables at first glance.
```

Testing and optimizing by picking the most appropriate mtry, node size and ntree.
```{r}
yhat.cluster3 = predict(cluster3_RF,newdata=test)
mean((yhat.cluster3-test$obesrate)^2) # MSE of random forest model performance on test data
mean((mean(test$obesrate)-test$obesrate)^2) # MSE of testing data set
mean((mean(train$obesrate)-test$obesrate)^2) # MSE of training data set

# the random forest model performed better than just guessing the mean obesity rate in both the test and train set.
```

