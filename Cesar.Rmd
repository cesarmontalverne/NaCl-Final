---
title: 'Cluster #2'
author: "Cesar Godoy"
date: "2022-12-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(plyr)
library(pROC)
library(plotly)
library(randomForest)
library(mltools)
library(rio)
library(caret)
library(ROCR)
library(tidyverse)
library(data.table)
library(rpart)
library(psych)
library(pROC)
library(rpart.plot)
library(rattle)
```

## Cluster 2 Analysis
Cluster 2 has very few data points. Because of this I will use 85% of the data as training data and tuning and the rest will be used for testing.

This was a very interesting cluster as it was supposed to concentrate counties with a really high degree of the population with access to groceries and counties with low access to gyms. Initially I believed these features wouldn't be correlated negatively, but it seems they are, at least in the national level.
```{r}
clusterdata <- read.csv("cluster_2_data.csv")
clusterdata <- clusterdata %>% select(-c(clusterNum, state, county))
summary(clusterdata)

```


Create test, tune and training sets 
```{r}
set.seed(22)
part_index_1 <- createDataPartition(clusterdata$obesrate,
                                           times=1,
                                           p = 0.85,
                                           groups=1,
                                           list=FALSE)

train <- clusterdata[part_index_1,]
test <- clusterdata[-part_index_1, ]

```

Calculate the initial mtry level 
```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}

mytry_tune(clusterdata)
```

```{r}
set.seed(22)
cluster_RF = randomForest(obesrate~., train, ntree = 500,
                            mtry = 3,            
                            replace = TRUE,      
                            sampsize = 100,      
                            nodesize = 5,        
                            importance = TRUE,   
                            proximity = FALSE,    
                            norm.votes = TRUE,   
                            keep.forest = TRUE,  
                            keep.inbag = TRUE)   
```

```{r}
importance <- as.data.frame(cluster_RF$importance)
importance <- importance %>% mutate("%IncMSE"=round((`%IncMSE`),6))
importance
cluster2 = predict(cluster_RF,newdata=test)
mean((cluster2-test$obesrate)^2)
mean((mean(train$obesrate)-test$obesrate)^2)
```
The mean squared error of the sample was really low. This is to be expected, since we clustered data into four groups that are similar in determining aspects to obesity rate.

If instead of training the data set, we just guessed the average obesity rate of the training data the MSE would be of 0.01798417. Using the data to train it using the random forest method, gives us a 0.01233427 MSE, which is about 30% lower. 

Given there's such few data points, especially for the testing portion of this, I believe that this is a good result.

Non-surprisingly, the rate of people with diabetes in the county was the most important predictor followed by median income in the county. Next, the measure of fast food restaurants in the county and income basically tied as relevant measures in determining the obesity rate of the population.